"""
Chapter 8-1: RAG Pipeline - ìˆ˜ì§‘ ì—”ë“œí¬ì¸íŠ¸ (Ingest)

Markdown ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³ , ì²­í¬ë¡œ ë¶„í• í•œ í›„, ì„ë² ë”©ì„ ìƒì„±í•˜ì—¬ MongoDBì— ì €ì¥í•©ë‹ˆë‹¤.

[Langchain ì‚¬ìš© ë²”ìœ„]
- MarkdownHeaderTextSplitter: í—¤ë” ê¸°ì¤€ ì„¹ì…˜ ë¶„í• 
- RecursiveCharacterTextSplitter: ì„¸ë¶€ ì²­í¬ ë¶„í• 

[OpenAI SDK ì§ì ‘ ì‚¬ìš©]
- ì„ë² ë”© ìƒì„±: client.embeddings.create()

ì‹¤í–‰: python chapter_8-1/ingest.py
"""

import os
from pathlib import Path

from dotenv import load_dotenv
from langchain_text_splitters import (
    MarkdownHeaderTextSplitter,
    RecursiveCharacterTextSplitter,
)
from openai import OpenAI
from pymongo import MongoClient
from pymongo.collection import Collection

load_dotenv()

# ìƒ˜í”Œ ë°ì´í„° ê²½ë¡œ
SAMPLE_MD_PATH = Path(__file__).parent.parent / "assets" / "sample.md"


def split_document(text: str) -> list[dict]:
    """
    Markdown ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

    1ë‹¨ê³„: MarkdownHeaderTextSplitterë¡œ í—¤ë” ê¸°ì¤€ ì„¹ì…˜ ë¶„í• 
    2ë‹¨ê³„: RecursiveCharacterTextSplitterë¡œ ì„¸ë¶€ ì²­í¬ ë¶„í• 

    Args:
        text: Markdown ë¬¸ì„œ í…ìŠ¤íŠ¸

    Returns:
        list[dict]: ì²­í¬ ë¦¬ìŠ¤íŠ¸ [{"content": ..., "metadata": ...}, ...]
    """
    # STEP 1: í—¤ë” ê¸°ì¤€ ë¶„í• 
    header_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=[
            ("#", "h1"),
            ("##", "h2"),
            ("###", "h3"),
        ]
    )
    sections = header_splitter.split_text(text)

    # STEP 2: ì„¸ë¶€ ì²­í¬ ë¶„í• 
    char_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
    )

    chunks = []
    for section in sections:
        # ì„¹ì…˜ë³„ë¡œ ì„¸ë¶€ ë¶„í• 
        sub_chunks = char_splitter.split_text(section.page_content)
        for sub_chunk in sub_chunks:
            chunks.append(
                {
                    "content": sub_chunk,
                    "metadata": section.metadata,
                }
            )

    return chunks


def store_chunks(
    collection: Collection,
    client: OpenAI,
    chunks: list[dict],
    source_name: str = "sample.md",
) -> int:
    documents = []
    total = len(chunks)

    for i, chunk in enumerate(chunks, 1):
        print(f"   ğŸ§  ì„ë² ë”© ìƒì„± ì¤‘... ({i}/{total})", end="\r")

        # ì„ë² ë”© ìƒì„±
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=chunk["content"],
        )
        embedding = response.data[0].embedding

        # ë¬¸ì„œ êµ¬ì„±
        doc = {
            "content": chunk["content"],
            "metadata": {
                **chunk["metadata"],
                "source": source_name,
            },
            "content_vector": embedding,
        }
        documents.append(doc)

    print()  # ì¤„ë°”ê¿ˆ

    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆë¡œ ì‚½ì… (ë°ëª¨ìš©)
    collection.delete_many({})
    collection.insert_many(documents)

    return len(documents)


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================


def main():
    # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    mongo_client = MongoClient(os.getenv("MONGODB_URI"))
    collection = mongo_client["hackers"]["rag_demo"]

    # STEP 1: ë¬¸ì„œ ë¡œë“œ
    raw_text = SAMPLE_MD_PATH.read_text(encoding="utf-8")
    print(f"   ğŸ“ Markdown ë¡œë“œ ì™„ë£Œ ({len(raw_text):,}ì)")

    # STEP 2: ë¬¸ì„œ ë¶„í• 
    chunks = split_document(raw_text)
    print(f"   ğŸ”ª ì²­í¬ ë¶„í•  ì™„ë£Œ: {len(chunks)}ê°œ")

    # STEP 3: ì„ë² ë”© ìƒì„± + MongoDB ì €ì¥
    stored_count = store_chunks(
        collection,
        openai_client,
        chunks,
        source_name=SAMPLE_MD_PATH.name,
    )
    print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ! ({stored_count}ê°œ ì²­í¬)")


if __name__ == "__main__":
    main()
