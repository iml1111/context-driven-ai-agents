"""
Chapter 8-1: RAG Pipeline - ì§ˆì˜ ì—”ë“œí¬ì¸íŠ¸ (Query)

ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„ Vector Searchë¡œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ ,
LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

ì‹¤í–‰: python chapter_8-1/query.py
"""

import os
import time

from dotenv import load_dotenv
from openai import OpenAI
from pymongo import MongoClient

load_dotenv()

SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ê·œì¹™:
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
2. ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì–¸ê¸‰ë˜ì§€ ì•Šë”ë¼ë„, ë§¥ë½ìƒ ì¶”ë¡  ê°€ëŠ¥í•œ ë‚´ìš©ì€ ë‹µë³€í•´ë„ ë©ë‹ˆë‹¤.
3. ë‹µë³€ì€ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
4. ë‹µë³€ ì‹œ ì°¸ê³ í•œ ì»¨í…ìŠ¤íŠ¸ ë²ˆí˜¸ë¥¼ [1], [2] í˜•ì‹ìœ¼ë¡œ ë³¸ë¬¸ì— ì¸ìš© í‘œì‹œí•˜ì„¸ìš”."""


def main():
    # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    mongo_client = MongoClient(os.getenv("MONGODB_URI"))
    collection = mongo_client["hackers"]["rag_demo"]

    # í•˜ë“œì½”ë”©ëœ ì§ˆë¬¸
    query = "ë°ì´í„°ë¶ ê¸°ì¤€ ë‚˜ë£¨í† ì™€ ì‚¬ìŠ¤ì¼€ì˜ ìŠ¤íƒœë¯¸ë‚˜ ìˆ˜ì¹˜ëŠ” ê°ê° ì–¼ë§ˆì´ë©°, ì´ ì°¨ì´ê°€ ìµœì¢…ì „ì—ì„œ ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì³¤ë‚˜ìš”?"
    print(f"\nğŸ™‹ ì§ˆë¬¸: {query}")

    # STEP 1: Vector Searchë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    print("\nğŸ” ê²€ìƒ‰ ì¤‘...")
    t0 = time.perf_counter()

    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    embedding_response = openai_client.embeddings.create(
        model="text-embedding-3-small",
        input=query,
    )
    query_embedding = embedding_response.data[0].embedding

    # MongoDB $vectorSearch
    pipeline = [
        {
            "$vectorSearch": {
                "index": "vector_index",
                "path": "content_vector",
                "queryVector": query_embedding,
                "numCandidates": 40,
                "limit": 5,
            }
        },
        {
            "$project": {
                "content": 1,
                "metadata": 1,
                "score": {"$meta": "vectorSearchScore"},
            }
        },
    ]
    documents = list(collection.aggregate(pipeline))

    search_time = time.perf_counter() - t0
    print(f"   â†’ {len(documents)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨ ({search_time:.2f}ì´ˆ)")

    # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°
    if documents:
        print("\nğŸ“„ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸:")
        for i, doc in enumerate(documents, 1):
            content = doc.get("content", "")
            score = doc.get("score", 0)
            preview = content.replace("\n", " ")[:80]
            print(f"   [{i}] (score: {score:.4f}) {preview}...")

    # STEP 2: ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
    if not documents:
        context = "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    else:
        context_parts = []
        for i, doc in enumerate(documents, 1):
            content = doc.get("content", "")
            score = doc.get("score", 0)
            context_parts.append(f"[{i}] (score: {score:.4f})\n{content}")
        context = "\n\n".join(context_parts)

    # STEP 3: LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
    print("\nğŸ’¡ ë‹µë³€ ìƒì„± ì¤‘...")
    t0 = time.perf_counter()

    response = openai_client.chat.completions.create(
        model="gpt-5.1",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": f"ì»¨í…ìŠ¤íŠ¸:\n{context}\n\nì§ˆë¬¸: {query}"},
        ],
    )
    answer = response.choices[0].message.content

    gen_time = time.perf_counter() - t0
    print(f"   â†’ ë‹µë³€ ìƒì„± ì™„ë£Œ ({gen_time:.2f}ì´ˆ)")

    print(f"\nâœ… ë‹µë³€:\n{answer}")


if __name__ == "__main__":
    main()
