"""
Chapter 13: Report - í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±

ë‹¨ì¼ ë²„ì „ ë¦¬í¬íŠ¸ ë° A/B ë¹„êµ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

from pydantic import BaseModel

from evaluators.calibration import CalibrationResult


# ============================================================
# ë°ì´í„° ëª¨ë¸
# ============================================================


class CaseResult(BaseModel):
    """ì¼€ì´ìŠ¤ë³„ í‰ê°€ ê²°ê³¼"""

    case_id: int
    claim: str
    ground_truth: str
    predicted: str
    is_correct: bool
    confidence: float
    checklist_score: int  # 1-5
    evidence_score: int  # 1-5


class EvalReport(BaseModel):
    """ì¢…í•© í‰ê°€ ë¦¬í¬íŠ¸"""

    version_name: str  # ë²„ì „ ì´ë¦„ (Ver 1 - Baseline, Ver 2 - Improved)

    # ìš”ì•½ ì§€í‘œ
    total_cases: int
    accuracy: float  # L1: ì •í™•ë„ (0.0 ~ 1.0)
    avg_checklist_score: float  # L2: í‰ê·  ì²´í¬ë¦¬ìŠ¤íŠ¸ ì ìˆ˜ (1-5)
    avg_evidence_score: float  # L3: í‰ê·  ê·¼ê±° ì ìˆ˜ (1-5)
    calibration: CalibrationResult  # L4: Calibration ë¶„ì„

    # ìƒì„¸ ê²°ê³¼
    case_results: list[CaseResult]


# ============================================================
# ë¦¬í¬íŠ¸ ìƒì„±
# ============================================================


def generate_report(
    version_name: str,
    case_results: list[CaseResult],
    calibration: CalibrationResult,
) -> EvalReport:
    """
    ì¢…í•© í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±

    Args:
        version_name: ë²„ì „ ì´ë¦„
        case_results: ì¼€ì´ìŠ¤ë³„ í‰ê°€ ê²°ê³¼
        calibration: Calibration ë¶„ì„ ê²°ê³¼

    Returns:
        EvalReport: ì¢…í•© í‰ê°€ ë¦¬í¬íŠ¸
    """
    total = len(case_results)

    # L1: ì •í™•ë„
    correct_count = sum(1 for r in case_results if r.is_correct)
    accuracy = correct_count / total if total > 0 else 0.0

    # L2: í‰ê·  ì²´í¬ë¦¬ìŠ¤íŠ¸ ì ìˆ˜
    avg_checklist = (
        sum(r.checklist_score for r in case_results) / total if total > 0 else 0.0
    )

    # L3: í‰ê·  ê·¼ê±° ì ìˆ˜
    avg_evidence = (
        sum(r.evidence_score for r in case_results) / total if total > 0 else 0.0
    )

    return EvalReport(
        version_name=version_name,
        total_cases=total,
        accuracy=round(accuracy, 4),
        avg_checklist_score=round(avg_checklist, 2),
        avg_evidence_score=round(avg_evidence, 2),
        calibration=calibration,
        case_results=case_results,
    )


# ============================================================
# ë¦¬í¬íŠ¸ ì¶œë ¥
# ============================================================


def print_comparison_report(v1_report: EvalReport, v2_report: EvalReport) -> None:
    """A/B ë¹„êµ ë¦¬í¬íŠ¸ ì¶œë ¥"""
    print(f"\n{'='*60}")
    print(f"ğŸ“Š A/B ì„±ëŠ¥ ë¹„êµ: Ver 1 vs Ver 2")
    print(f"{'='*60}")

    # Ver 1 ìš”ì•½
    v1_correct = sum(1 for r in v1_report.case_results if r.is_correct)
    print(f"\n[{v1_report.version_name}]")
    print(f"  ì •í™•ë„: {v1_report.accuracy:.1%} ({v1_correct}/{v1_report.total_cases})")
    print(f"  ì²´í¬ë¦¬ìŠ¤íŠ¸ í’ˆì§ˆ: {v1_report.avg_checklist_score:.1f}/5")
    print(f"  ê·¼ê±° ì¶©ë¶„ì„±: {v1_report.avg_evidence_score:.1f}/5")
    print(f"  Calibration ECE: {v1_report.calibration.expected_calibration_error:.4f}")

    # Ver 2 ìš”ì•½
    v2_correct = sum(1 for r in v2_report.case_results if r.is_correct)
    print(f"\n[{v2_report.version_name}]")
    print(f"  ì •í™•ë„: {v2_report.accuracy:.1%} ({v2_correct}/{v2_report.total_cases})")
    print(f"  ì²´í¬ë¦¬ìŠ¤íŠ¸ í’ˆì§ˆ: {v2_report.avg_checklist_score:.1f}/5")
    print(f"  ê·¼ê±° ì¶©ë¶„ì„±: {v2_report.avg_evidence_score:.1f}/5")
    print(f"  Calibration ECE: {v2_report.calibration.expected_calibration_error:.4f}")

    # ê°œì„  íš¨ê³¼
    print(f"\n[ê°œì„  íš¨ê³¼]")

    acc_diff = (v2_report.accuracy - v1_report.accuracy) * 100
    checklist_diff = v2_report.avg_checklist_score - v1_report.avg_checklist_score
    evidence_diff = v2_report.avg_evidence_score - v1_report.avg_evidence_score
    ece_diff = v2_report.calibration.expected_calibration_error - v1_report.calibration.expected_calibration_error

    def format_diff(diff: float, unit: str = "", reverse: bool = False) -> str:
        """ì°¨ì´ê°’ í¬ë§·íŒ… (reverse=Trueë©´ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)"""
        if reverse:
            if diff < 0:
                return f"\033[92m{diff:+.2f}{unit} (ê°œì„ )\033[0m"  # ë…¹ìƒ‰
            elif diff > 0:
                return f"\033[91m{diff:+.2f}{unit} (ì•…í™”)\033[0m"  # ë¹¨ê°•
        else:
            if diff > 0:
                return f"\033[92m{diff:+.2f}{unit} (ê°œì„ )\033[0m"  # ë…¹ìƒ‰
            elif diff < 0:
                return f"\033[91m{diff:+.2f}{unit} (ì•…í™”)\033[0m"  # ë¹¨ê°•
        return f"{diff:+.2f}{unit}"

    print(f"  ì •í™•ë„: {format_diff(acc_diff, '%p')}")
    print(f"  ì²´í¬ë¦¬ìŠ¤íŠ¸: {format_diff(checklist_diff, 'ì ')}")
    print(f"  ê·¼ê±°: {format_diff(evidence_diff, 'ì ')}")
    print(f"  Calibration: {format_diff(ece_diff, '', reverse=True)}")

    # ì¼€ì´ìŠ¤ë³„ ë¹„êµ (ë¶ˆì¼ì¹˜ ì¼€ì´ìŠ¤)
    print(f"\n[ì¼€ì´ìŠ¤ë³„ ë¹„êµ - ë¶ˆì¼ì¹˜ í•­ëª©]")
    has_diff = False
    for r1, r2 in zip(v1_report.case_results, v2_report.case_results):
        if r1.is_correct != r2.is_correct:
            has_diff = True
            v1_status = "âœ…" if r1.is_correct else "âŒ"
            v2_status = "âœ…" if r2.is_correct else "âŒ"
            print(f"  Case {r1.case_id}: V1 {v1_status} â†’ V2 {v2_status}")
            print(f"    \"{r1.claim[:40]}...\"")
            print(f"    ì •ë‹µ: {r1.ground_truth}")

    if not has_diff:
        print("  (ëª¨ë“  ì¼€ì´ìŠ¤ì—ì„œ V1ê³¼ V2ì˜ ì •ì˜¤ë‹µì´ ë™ì¼)")


# ============================================================
# í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰
# ============================================================

if __name__ == "__main__":
    from evaluators.calibration import CalibrationResult

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_cases = [
        CaseResult(
            case_id=1,
            claim="ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì´ë‹¤",
            ground_truth="TRUE",
            predicted="TRUE",
            is_correct=True,
            confidence=0.95,
            checklist_score=5,
            evidence_score=4,
        ),
        CaseResult(
            case_id=2,
            claim="ë§Œë¦¬ì¥ì„±ì€ ìš°ì£¼ì—ì„œ ë³´ì¸ë‹¤",
            ground_truth="FALSE",
            predicted="TRUE",
            is_correct=False,
            confidence=0.80,
            checklist_score=4,
            evidence_score=3,
        ),
    ]

    calibration = CalibrationResult(
        expected_calibration_error=0.12,
        overconfident_count=1,
        underconfident_count=0,
        total_cases=2,
        bins=[],
    )

    report = generate_report("Ver 1 - Baseline (gpt-4o)", test_cases, calibration)
    print(f"âœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report.version_name} ({report.total_cases}ê°œ ì¼€ì´ìŠ¤)")
