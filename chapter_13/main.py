"""
Chapter 13: Agent Workflow Evaluation Pipeline

ë‘ ë²„ì „ì˜ íŒ©íŠ¸ì²´ì»¤ë¥¼ ë™ì¼ ë°ì´í„°ì…‹ìœ¼ë¡œ í‰ê°€í•˜ì—¬ ì„±ëŠ¥ ë¹„êµí•©ë‹ˆë‹¤.

ì›Œí¬í”Œë¡œìš°:
1. ë°ì´í„°ì…‹ ë¡œë“œ (eval_data.jsonl)
2. Ver 1 (gpt-4o, ì›ë³¸) í‰ê°€
3. Ver 2 (gpt-5.1, ê°œì„ ) í‰ê°€
4. A/B ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±

í•™ìŠµ ëª©í‘œ:
- ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ ë¹„ê²°ì •ì  íŠ¹ì„± ì´í•´
- ì¤‘ê°„ ì‚°ì¶œë¬¼(ì²´í¬ë¦¬ìŠ¤íŠ¸) + ìµœì¢… ì¶œë ¥(verdict) ë‹¤ì¸µ í‰ê°€
- LLM-as-Judge íŒ¨í„´ êµ¬í˜„
- Before/After ì„±ëŠ¥ ë¹„êµë¥¼ í†µí•œ ê°œì„  ì¸¡ì •
"""

from dataset import EvalCase, load_dataset
from evaluators import (
    AccuracyEvaluator,
    CalibrationAnalyzer,
    ChecklistJudge,
    EvidenceJudge,
)
from fact_checkers import FactCheckerV1, FactCheckerV2
from report import CaseResult, EvalReport, generate_report, print_comparison_report


# ============================================================
# ë‹¨ì¼ ì—ì´ì „íŠ¸ í‰ê°€
# ============================================================


def evaluate_agent(
    agent,
    dataset: list[EvalCase],
    version_name: str,
    accuracy_evaluator: AccuracyEvaluator,
    checklist_judge: ChecklistJudge,
    evidence_judge: EvidenceJudge,
    calibration_analyzer: CalibrationAnalyzer,
) -> EvalReport:
    """
    ë‹¨ì¼ ì—ì´ì „íŠ¸ í‰ê°€ ì‹¤í–‰

    Args:
        agent: íŒ©íŠ¸ì²´ì»¤ ì—ì´ì „íŠ¸ (V1 ë˜ëŠ” V2)
        dataset: í‰ê°€ ë°ì´í„°ì…‹
        version_name: ë²„ì „ ì´ë¦„
        accuracy_evaluator: L1 ì •í™•ë„ í‰ê°€ê¸°
        checklist_judge: L2 ì²´í¬ë¦¬ìŠ¤íŠ¸ í‰ê°€ê¸°
        evidence_judge: L3 ê·¼ê±° í‰ê°€ê¸°
        calibration_analyzer: L4 Calibration ë¶„ì„ê¸°

    Returns:
        EvalReport: í‰ê°€ ë¦¬í¬íŠ¸
    """
    print(f"ğŸ”„ {version_name} í‰ê°€ ì‹œì‘...")

    case_results = []
    calibration_data = []  # (confidence, is_correct) íŠœí”Œ

    for i, case in enumerate(dataset, 1):
        print(f"\nğŸ“ Case {i}/{len(dataset)}: {case.claim[:40]}...")

        # ì—ì´ì „íŠ¸ ì‹¤í–‰
        result = agent.run(case.claim)

        # L1: Accuracy í‰ê°€
        acc_result = accuracy_evaluator.evaluate(result.verdict, case.ground_truth.value)

        # L2: Checklist Quality í‰ê°€
        checklist_score = checklist_judge.evaluate(case.claim, result.checklist)

        # L3: Evidence Quality í‰ê°€
        evidence_score = evidence_judge.evaluate(
            claim=case.claim,
            verdict=result.verdict,
            confidence=result.confidence,
            evidence=result.evidence,
            checklist=result.checklist,
        )

        # ê²°ê³¼ ì €ì¥
        case_results.append(
            CaseResult(
                case_id=i,
                claim=case.claim,
                ground_truth=case.ground_truth.value,
                predicted=acc_result.predicted,
                is_correct=acc_result.is_correct,
                confidence=result.confidence,
                checklist_score=checklist_score.score,
                evidence_score=evidence_score.score,
            )
        )

        # Calibrationìš© ë°ì´í„° ìˆ˜ì§‘
        calibration_data.append((result.confidence, acc_result.is_correct))

        # ì§„í–‰ ìƒí™© ì¶œë ¥
        status = "âœ…" if acc_result.is_correct else "âŒ"
        print(f"  {status} {acc_result.predicted} (ì •ë‹µ: {case.ground_truth.value})")
        print(f"  ğŸ“‹ Checklist: {checklist_score.score}/5 | ğŸ“„ Evidence: {evidence_score.score}/5")

    # L4: Calibration ë¶„ì„
    calibration = calibration_analyzer.analyze(calibration_data)

    return generate_report(version_name, case_results, calibration)


# ============================================================
# A/B ë¹„êµ ì‹¤í–‰
# ============================================================


def run_ab_comparison():
    """A/B ë¹„êµ í‰ê°€ ì‹¤í–‰"""
    print("=" * 60)
    print("ğŸ“Š Chapter 13: Fact-Checker A/B Evaluation")
    print("=" * 60)

    # ë°ì´í„°ì…‹ ë¡œë“œ
    dataset = load_dataset("eval_data.jsonl")
    print(f"\nğŸ“ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset)}ê°œ ì¼€ì´ìŠ¤")

    # í‰ê°€ê¸° ì´ˆê¸°í™”
    accuracy_evaluator = AccuracyEvaluator()
    checklist_judge = ChecklistJudge()
    evidence_judge = EvidenceJudge()
    calibration_analyzer = CalibrationAnalyzer()

    # Ver 1 í‰ê°€ (Baseline)
    v1_agent = FactCheckerV1()
    v1_report = evaluate_agent(
        v1_agent,
        dataset,
        "Ver 1 - Baseline (gpt-4o)",
        accuracy_evaluator,
        checklist_judge,
        evidence_judge,
        calibration_analyzer,
    )

    # Ver 2 í‰ê°€ (Improved)
    v2_agent = FactCheckerV2()
    v2_report = evaluate_agent(
        v2_agent,
        dataset,
        "Ver 2 - Improved (gpt-5.1)",
        accuracy_evaluator,
        checklist_judge,
        evidence_judge,
        calibration_analyzer,
    )

    # A/B ë¹„êµ ë¦¬í¬íŠ¸
    print_comparison_report(v1_report, v2_report)

    print(f"\n{'='*60}")
    print("ğŸ‰ A/B í‰ê°€ ì™„ë£Œ!")
    print("=" * 60)


# ============================================================
# Entry Point
# ============================================================

if __name__ == "__main__":
    run_ab_comparison()
